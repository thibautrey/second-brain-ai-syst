/**
 * Chat Controller
 *
 * Handles chat requests with SSE streaming responses
 *
 * Refactored into focused services for maintainability:
 * - chat-provider.ts: Provider configuration and caching
 * - chat-tools.ts: Tool extraction and execution
 * - chat-token-recovery.ts: Token validation and recovery strategies
 * - chat-context.ts: Memory retrieval and context building
 * - telegram-chat.ts: Telegram-specific message processing
 *
 * OPTIMIZATIONS IMPLEMENTED:
 * - Opt 1: Parallel Processing Pipeline
 * - Opt 2: Aggressive Response Caching
 * - Opt 4: Provider cache with 5-minute TTL
 * - Opt 5: Single LLM call for classification+assessment AFTER response
 * - Opt 7: Parallel fetch of provider + memory search
 * - Opt 8: Speculative Execution
 * - Opt 9: Precomputed Memory Indices
 */

import { Response, NextFunction } from "express";
import { AuthRequest } from "../middlewares/auth.middleware.js";
import { IntentRouterService } from "../services/intent-router.js";
import { notificationService } from "../services/notification.js";
import prisma from "../services/prisma.js";
import OpenAI from "openai";
import { flowTracker } from "../services/flow-tracker.js";
import { randomBytes } from "crypto";
import { toolExecutorService } from "../services/tool-executor.js";
import {
  generateToolExecutionSummary,
  createToolMetadata,
} from "../services/sanitize-tool-results.js";
import { getDefaultMaxTokens } from "./ai-settings.controller.js";
import { responseCacheService } from "../services/response-cache.js";
import { speculativeExecutor } from "../services/speculative-executor.js";
// New service imports
import { getChatProvider } from "../services/chat-provider.js";
import {
  extractAllTextToolCalls,
  executeTextToolCalls,
  executeFunctionCalls,
} from "../services/chat-tools.js";
import { recoverFromMaxTokensError } from "../services/chat-token-recovery.js";
import {
  buildMemoryContext,
  getUserContextData,
  buildSystemPrompt,
  prepareSystemPrompt,
  CHAT_SYSTEM_PROMPT,
} from "../services/chat-context.js";
import { processTelegramMessage } from "../services/telegram-chat.js";

type ToolFunctionDefinition = {
  name: string;
  description?: string;
  parameters?: Record<string, any>;
};

type LlmTool = {
  type: "function";
  function: ToolFunctionDefinition;
};

type ChatMessageParam = {
  role: "system" | "user" | "assistant" | "tool";
  content: string | null;
  tool_calls?: any[];
  tool_call_id?: string;
  name?: string;
};

const intentRouter = new IntentRouterService();

const CHAT_SYSTEM_PROMPT = `You are Second Brain, a concise and intelligent personal assistant.
You help the user organize their thoughts, recall memories, and answer their questions.
You have access to the user's memories to personalize your responses.

AVAILABLE TOOLS:
You have access to tools that you MUST use via the function calling mechanism (tool_calls).
NEVER produce curl, http, or JSON commands as raw textâ€”ALWAYS use the provided tools.
Never delete/disable/overwrite tasks, todos, or scheduled items without explicit user confirmation. Default to reading/listing or creating; edits/deletes require confirmation.

- curl: For HTTP requests (weather, web APIs, etc.). Use it whenever the user asks for web-based information.
- brave_search: Search the public web via Brave Search (requires API key BRAVE_SEARCH_API_KEY). If missing, ask the user to provide it and store it via secrets before retrying.
- todo: Manage the user's to-dos from end to end (create, get, list, update, complete, delete). Use update to change priority/date/description and delete to remove items. You may modify or delete existing tasks.
- notification: Send reminders and notifications (send, schedule, list, mark_read).
- scheduled_task: Schedule tasks with full edit/delete control (create, get, list, update, enable, disable, delete, execute_now). You can modify or delete scheduled tasks after creation.
- For recurring conditional alerts (e.g., ticket releases, price drops), create a scheduled_task with actionType=WATCH_RESOURCE and an interval or cron. Put the URL to poll in actionPayload.fetch, define the rule in actionPayload.condition (json/text/status + op + value/pattern), and set actionPayload.notify with the title/message to fire when the condition is met.
- user_context: Retrieve user information from memory (location, preferences, facts).
- user_profile: RECORD important personal information about the user (name, job, location, preferences, relationships, etc.). Use this tool whenever the user shares structural personal data.
- long_running_task: Use for long-running work (deep research, complex analysis, etc.). Use it when a request will take more than a few minutes.

ðŸ”¥ PERSISTENCE AND RESILIENCE - VERY IMPORTANT:
You must ALWAYS try to respond to the user, even if a tool fails.
Never give up after one failure. You have multiple chances to succeed.

WHEN A TOOL FAILS:
1. Analyze the error (invalid API key? service unavailable? wrong parameters?)
2. Try an ALTERNATIVE APPROACH:
   - If an API fails â†’ try another provider.
   - If a service needs an API key â†’ use a free option that requires no authentication.
   - If an endpoint is down â†’ try a different endpoint.
3. Keep going until you find a working solution.
4. Try at most 2 alternative providers/approaches. If everything fails, return the best partial result plus the last errorâ€”do not loop indefinitely.
5. Inform the user ONLY after you have exhausted all alternatives.

EXAMPLES OF RESILIENCE:
- Error "API key invalid" â†’ Donâ€™t quit!
- Error "Service unavailable" â†’ Wait and retry, or switch to an alternative.
- Error 404 â†’ Check and fix the URL, or use another source.

USER PROFILE:
IMPORTANT: When the user shares important personal details (name, job, location, preferences, loved ones, etc.), IMMEDIATELY use user_profile to store them.
- Those details stay available in your context.
- You no longer have to search memory for profile facts.
- Examples: "My name is Jean" â†’ user_profile action=update name="Jean"
- "I work at Google" â†’ user_profile action=update company="Google"
- "My wife's name is Marie" â†’ user_profile action=update relationships=[{name: "Marie", relation: "wife"}]

LONG RUNNING TASKS:
Use long_running_task when:
- The user asks for work that will take time (extensive research, complex analysis).
- A task requires multiple steps.
- You need to execute something that may take minutes or hours.
- The user wants background work.

Workflow for creating a long-running task:
1. Create the task with action="create" (name, description, objective required).
2. Add the steps with action="add_steps" (taskId + steps array).
3. Start it with action="start" (taskId).

You can later check progress with action="get_progress" or "get_report".

WHEN TO USE TOOLS:
- Weather questions â†’ curl against a weather API or site.
- Task management â†’ todo.
- Reminders â†’ notification or scheduled_task.
- User-related queries (search) â†’ user_context.
- Capturing personal data â†’ user_profile.

VERIFYING TOOL RESULTS - VERY IMPORTANT:
After every tool invocation, you MUST verify that the action succeeded:
- After creating a task â†’ todo action=get with the returned ID to confirm it exists.
- After creating a notification â†’ confirm the response has success=true and includes an ID.
- After scheduling a task â†’ scheduled_task action=get to verify it was created.
- After updating the user profile â†’ user_profile action=get to check the changes.
- After an HTTP request â†’ inspect the status code and returned data.

If verification fails:
1. Inform the user about the issue.
2. Try to correct or retry the operation.
3. Never confirm success without verification.

Examples of a correct workflow:
- "Create a task" â†’ todo create â†’ todo get to verify â†’ "Task created successfully."
- "Remind me tomorrow" â†’ notification schedule â†’ verify success=true â†’ "Reminder scheduled."

IMPORTANT INSTRUCTIONS:
- Respond in a VERY CONCISE and helpful way.
- Answer in the user's language. Keep replies to 3 sentences or fewer unless the user explicitly asks for more detail.
- Never expose raw tool payloads or JSON; present clean, human-readable summaries only.
- For simple factual statements (sharing personal information), reply with "Understood" or a very short acknowledgement.
- Do NOT add a question at the end of every replyâ€”that is repetitive and annoying.
- Ask a question ONLY when it is genuinely needed or you need clarification.
- Use memory context when relevant, but do so subtly.
- When the user requests something, answer directly without unnecessary steps.
- Be natural: a friend does not ask a question after every statement.
- NEVER show JSON or curl to the userâ€”use the tools, then give a natural response.`;

/**
 * Helper: Extract ALL tool calls from text content
 * Handles multiple consecutive tool calls like: user_profile{...}scheduled_task{...}
 */
function extractAllTextToolCalls(content: string): Array<{
  toolId: string;
  params: Record<string, any>;
  start: number;
  end: number;
}> {
  const results: Array<{
    toolId: string;
    params: Record<string, any>;
    start: number;
    end: number;
  }> = [];

  // Pattern to find toolName{...} anywhere in the text
  const toolPattern =
    /(user_profile|scheduled_task|todo|notification|curl|user_context)\s*(\{)/gi;

  let match;
  while ((match = toolPattern.exec(content)) !== null) {
    const toolId = match[1].toLowerCase();
    const startPos = match.index;
    const jsonStart = match.index + match[0].length - 1; // Position of opening {

    // Find matching closing brace
    let braceCount = 1;
    let jsonEnd = jsonStart + 1;

    while (jsonEnd < content.length && braceCount > 0) {
      if (content[jsonEnd] === "{") braceCount++;
      if (content[jsonEnd] === "}") braceCount--;
      jsonEnd++;
    }

    if (braceCount === 0) {
      try {
        const jsonStr = content.substring(jsonStart, jsonEnd);
        const params = JSON.parse(jsonStr);

        // Verify it has an action field (real tool call)
        if (params.action && typeof params.action === "string") {
          results.push({
            toolId,
            params,
            start: startPos,
            end: jsonEnd,
          });
        }
      } catch (e) {
        // JSON parse failed, skip this one
      }
    }
  }

  return results;
}

/**
 * POST /api/chat
 * Stream chat response using SSE
 *
 * OPTIMIZED FLOW:
 * 1. PARALLEL: Memory search + Provider fetch (no pre-classification!)
 * 2. Stream LLM response to user
 * 3. ASYNC (after response): Unified analysis (classification + value assessment)
 */
export async function chatStream(
  req: AuthRequest,
  res: Response,
  next: NextFunction,
) {
  const userId = req.userId;
  if (!userId) {
    return res.status(401).json({ error: "Unauthorized" });
  }

  const { message, messages: previousMessages } = req.body;
  if (!message || typeof message !== "string") {
    return res.status(400).json({ error: "Message is required" });
  }

  // Ensure previousMessages is always an array
  const validPreviousMessages = Array.isArray(previousMessages)
    ? previousMessages
    : [];

  const flowId = randomBytes(8).toString("hex");
  const startTime = Date.now();

  // Start flow tracking
  flowTracker.startFlow(flowId, "chat");
  flowTracker.trackEvent({
    flowId,
    stage: "chat_received",
    service: "ChatController",
    status: "started",
    data: { messageLength: message.length },
  });

  // Set SSE headers
  res.setHeader("Content-Type", "text/event-stream");
  res.setHeader("Cache-Control", "no-cache");
  res.setHeader("Connection", "keep-alive");
  res.setHeader("X-Accel-Buffering", "no");

  const messageId = `msg_${Date.now()}`;

  try {
    // Send start event
    res.write(`data: ${JSON.stringify({ type: "start", messageId })}\n\n`);

    // Record query for speculative execution patterns (Opt 8)
    speculativeExecutor.recordQuery(userId, message);

    // Check for speculative pre-fetched results first (Opt 8)
    const prefetchedResults = speculativeExecutor.getPrefetchedResults(
      userId,
      message,
    );

    // 1. PARALLEL PROCESSING PIPELINE (Opt 1, 2, 4, 7, 9)
    // Fetch everything in parallel: memories, provider, user context, conversation context
    const parallelStart = Date.now();

    const [
      memorySearchResult,
      providerResult,
      userContext,
      conversationContext,
    ] = await Promise.all([
      // Memory search - use prefetched if available, or optimized retrieval
      prefetchedResults
        ? Promise.resolve(prefetchedResults)
        : optimizedRetrieval.fastSearch(userId, message, 5).catch((error) => {
            console.warn("Optimized search failed, trying standard:", error);
            return memorySearchService
              .semanticSearch(userId, message, 5)
              .catch((err) => {
                console.warn(
                  "Memory search failed, continuing without context:",
                  err,
                );
                return { results: [], error: err };
              });
          }),
      // Provider fetch (using cache - Opt 4)
      getChatProvider(userId),
      // User context from precomputed index (Opt 9)
      precomputedMemoryIndex.getOrComputeContext(userId).catch(() => null),
      // Conversation context from cache (Opt 2)
      Promise.resolve(responseCacheService.getConversationContext(userId)),
    ]);

    const parallelDuration = Date.now() - parallelStart;

    // Start speculative pre-fetch for likely follow-ups (Opt 8 - non-blocking)
    const userTopics = userContext?.recentTopics || [];
    speculativeExecutor
      .speculativeFetch(userId, message, userTopics)
      .catch(() => {});

    // 2. Process memory search results (already fetched in parallel)
    let memoryContext: string[] = [];
    // Handle both optimizedRetrieval results and memorySearchService results
    const searchResults = Array.isArray(memorySearchResult)
      ? memorySearchResult.map((r) => ({
          memory: { createdAt: r.createdAt, content: r.content },
          score: r.certainty,
        }))
      : "results" in memorySearchResult
        ? memorySearchResult.results
        : [];

    // Ensure searchResults is always an array
    const validSearchResults = Array.isArray(searchResults)
      ? searchResults
      : [];

    if ("error" in memorySearchResult && !Array.isArray(memorySearchResult)) {
      flowTracker.trackEvent({
        flowId,
        stage: "memory_search",
        service: "MemorySearchService",
        status: "failed",
        duration: parallelDuration,
        error:
          memorySearchResult.error instanceof Error
            ? memorySearchResult.error.message
            : "Unknown error",
        decision:
          "Recherche mÃ©moire Ã©chouÃ©e. Continuation sans contexte mÃ©moire.",
      });
    } else {
      memoryContext = validSearchResults.map(
        (r: any) =>
          `[MÃ©moire du ${new Date(r.memory?.createdAt || r.createdAt).toLocaleDateString()}]: ${r.memory?.content || r.content}`,
      );

      const avgScore =
        validSearchResults.length > 0
          ? validSearchResults.reduce(
              (sum: number, r: any) => sum + (r.score || r.certainty || 0),
              0,
            ) / validSearchResults.length
          : 0;
      const memoryDecision =
        `${validSearchResults.length} mÃ©moire(s) pertinente(s) trouvÃ©e(s). ` +
        `Score moyen: ${(avgScore * 100).toFixed(1)}%.`;

      flowTracker.trackEvent({
        flowId,
        stage: "memory_search",
        service: "MemorySearchService",
        status: "success",
        duration: parallelDuration,
        data: {
          resultsFound: validSearchResults.length,
          query: message,
          topResults: validSearchResults
            .slice(0, 3)
            .map((r: any, i: number) => ({
              rank: i + 1,
              score: r.score || r.certainty,
              distance: r.distance,
              dateCreated: r.memory?.createdAt || r.createdAt,
            })),
          parallelExecution: true,
          usedPrefetch: !!prefetchedResults,
          userContextAvailable: !!userContext,
          conversationContextAvailable: !!conversationContext,
        },
        decision: memoryDecision,
      });
    }

    // 3. Check provider (already fetched in parallel - Optimization 4 & 7)
    if (!providerResult) {
      res.write(
        `data: ${JSON.stringify({
          type: "error",
          data: "Aucun fournisseur LLM configurÃ©. Allez dans ParamÃ¨tres > IA pour en ajouter un.",
        })}\n\n`,
      );
      res.write(`data: ${JSON.stringify({ type: "end", messageId })}\n\n`);
      flowTracker.completeFlow(flowId, "failed");
      return res.end();
    }

    const { provider, modelId, fallbackProvider, fallbackModelId } =
      providerResult;

    flowTracker.trackEvent({
      flowId,
      stage: "parallel_fetch_complete",
      service: "ChatController",
      status: "success",
      duration: parallelDuration,
      data: {
        providerName: provider.name,
        modelId,
        memoriesFound: validSearchResults.length,
        fromCache: parallelDuration < 50, // If very fast, likely from cache
      },
      decision: `Parallel fetch: Provider=${provider.name}, Model=${modelId}, MÃ©moires=${validSearchResults.length}`,
    });

    // 4. Create OpenAI client
    const openai = new OpenAI({
      apiKey: provider.apiKey,
      baseURL: provider.baseUrl || "https://api.openai.com/v1",
    });

    // 5. Build messages with memory context
    const systemPrompt =
      memoryContext.length > 0
        ? `${CHAT_SYSTEM_PROMPT}\n\nContexte des mÃ©moires pertinentes:\n${memoryContext.join("\n")}`
        : CHAT_SYSTEM_PROMPT;

    // 6. Get tool schemas for function calling (including generated tools)
    const toolSchemas =
      (await toolExecutorService.getToolSchemasWithGenerated(
        userId,
      )) as ToolFunctionDefinition[];

    // 7. HYBRID STREAMING MODE: Initial call to detect tool usage
    const llmStart = Date.now();
    // Build message history from previous messages + current message
    // Inject current date and user profile into system prompt
    const systemPromptWithContext = await injectContextIntoPrompt(
      systemPrompt,
      userId,
    );
    let messages: ChatMessageParam[] = [
      { role: "system", content: systemPromptWithContext },
    ];

    // Add previous conversation messages (if any)
    if (validPreviousMessages.length > 0) {
      for (const prevMsg of validPreviousMessages) {
        if (prevMsg.role === "user" && prevMsg.content) {
          messages.push({
            role: "user",
            content: prevMsg.content,
          });
        } else if (prevMsg.role === "assistant" && prevMsg.content) {
          messages.push({
            role: "assistant",
            content: prevMsg.content,
          });
        }
      }
    }

    // Add current user message
    messages.push({ role: "user", content: message });

    let fullResponse = "";
    let allToolResults: any[] = [];
    let sanitizationResults = new Map<string, any>();
    let iterationCount = 0;
    let consecutiveFailures = 0;
    const MAX_ITERATIONS = 30; // Allow more iterations for complex tasks
    const MAX_CONSECUTIVE_FAILURES = 5; // Stop if 5 failures in a row

    // Tool calling loop - generous iterations but with failure circuit breaker
    while (iterationCount < MAX_ITERATIONS) {
      iterationCount++;

      flowTracker.trackEvent({
        flowId,
        stage: `llm_call_iteration_${iterationCount}`,
        service: "OpenAI",
        status: "started",
        duration: 0,
        data: { messagesCount: messages.length },
      });

      // Validate max_tokens before making the request
      // Limit the context string to avoid massive estimation errors
      // (each message content up to ~1000 chars to prevent JSON tool results from inflating estimates)
      const messagesStr = messages
        .map((m) => {
          const content = typeof m.content === "string" ? m.content : "";
          // Limit each message to first 1000 chars to prevent tool result bloat from inflating estimate
          return content.substring(0, 1000);
        })
        .join(" ");

      // Get user's configured max tokens setting
      const userMaxTokens = await getDefaultMaxTokens(userId);

      const validation = validateMaxTokens(
        userMaxTokens,
        modelId,
        messages.length,
        messagesStr,
      );

      let maxTokensToUse = validation.maxTokens;

      if (validation.warning) {
        console.warn(`[TokenValidator] ${validation.warning}`);
        flowTracker.trackEvent({
          flowId,
          stage: `token_validation_warning_${iterationCount}`,
          service: "TokenValidator",
          status: "started",
          data: {
            warning: validation.warning,
            adjustedMaxTokens: maxTokensToUse,
          },
        });

        // Send warning notification to user when tokens are being reduced significantly
        if (maxTokensToUse < userMaxTokens * 0.5) {
          // Only notify if tokens reduced by more than 50%
          try {
            await notificationService.createNotification({
              userId,
              title: "Optimizing response length",
              message: `Your conversation is getting long. I'm optimizing to provide a response. If issues persist, try shortening your recent messages.`,
              type: "WARNING",
              channels: ["IN_APP"],
            });
          } catch (notifyError) {
            console.warn(
              "Failed to send token constraint notification:",
              notifyError,
            );
            // Don't block chat if notification fails
          }
        }
      }

      // Non-streaming call to detect tool usage
      try {
        const response = await openai.chat.completions.create({
          model: modelId,
          messages: messages as any,
          temperature: 0.7,
          max_tokens: maxTokensToUse, // Use validated max_tokens
          tools: toolSchemas.map((schema) => ({
            type: "function",
            function: schema,
          })) as LlmTool[],
          tool_choice: "auto",
          stream: false,
        });

        const assistantMessage = response.choices[0]?.message;

        if (!assistantMessage) {
          break;
        }

        // Detect if the model generated tool calls as text instead of using function calling
        // This can happen with some models/providers that don't support function calling well
        const textToolCalls =
          assistantMessage.content &&
          (!assistantMessage.tool_calls ||
            assistantMessage.tool_calls.length === 0)
            ? extractAllTextToolCalls(assistantMessage.content)
            : [];

        // Process ALL text-based tool calls found in the response
        if (textToolCalls.length > 0) {
          // Create a consistent tool_call_id for this batch of tools
          const batchToolCallId = `text_tool_batch_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
          const toolCallResults: Array<{
            toolCallId: string;
            toolId: string;
            success: boolean;
            data: any;
            error?: string;
          }> = [];

          // Execute each tool and collect results
          for (let i = 0; i < textToolCalls.length; i++) {
            const { toolId, params } = textToolCalls[i];
            const toolCallId = `${batchToolCallId}_${i}`;

            flowTracker.trackEvent({
              flowId,
              stage: `text_tool_call_executing_iteration_${iterationCount}`,
              service: "ChatController",
              status: "started",
              duration: 0,
              data: {
                toolId,
                action: params.action,
                toolIndex: i,
                totalCalls: textToolCalls.length,
              },
              decision: `ExÃ©cution de ${textToolCalls.length} appel(s) d'outil en texte. Traitement #${i + 1}: ${toolId}`,
            });

            // Execute the tool
            const toolExecutionStart = Date.now();
            const toolRequest = {
              toolId,
              action: params.action || "request",
              params,
            };

            // Send tool_call event to frontend (executing status)
            res.write(
              `data: ${JSON.stringify({
                type: "tool_call",
                data: {
                  id: toolCallId,
                  toolName: toolId,
                  action: params.action,
                  status: "executing",
                  startTime: toolExecutionStart,
                },
              })}\n\n`,
            );

            try {
              // Create callback for generation steps (only for generate_tool)
              const executionOptions =
                toolId === "generate_tool"
                  ? {
                      onGenerationStep: (step: any) => {
                        res.write(
                          `data: ${JSON.stringify({
                            type: "tool_generation",
                            data: {
                              ...step,
                              toolCallId,
                            },
                          })}\n\n`,
                        );
                      },
                    }
                  : undefined;

              const toolResult = await toolExecutorService.executeTool(
                userId,
                toolRequest,
                executionOptions,
              );

              // Sanitize the tool result before storing
              const sanitizationResult = sanitizeToolResult(toolResult.data);
              sanitizationResults.set(`${toolId}_${i}`, sanitizationResult);

              // Store sanitized version of tool result
              const sanitizedToolResult = {
                ...toolResult,
                data: sanitizationResult.cleaned,
                _sanitized: sanitizationResult.hasSensitiveData,
                _redactionCount: sanitizationResult.redactedCount,
              };

              allToolResults.push(sanitizedToolResult);

              // Track this result for message building
              toolCallResults.push({
                toolCallId,
                toolId,
                success: toolResult.success,
                data: toolResult.data,
                error: toolResult.error,
              });

              // Send tool_call event to frontend (success/error status)
              res.write(
                `data: ${JSON.stringify({
                  type: "tool_call",
                  data: {
                    id: toolCallId,
                    toolName: toolId,
                    action: params.action,
                    status: toolResult.success ? "success" : "error",
                    startTime: toolExecutionStart,
                    endTime: Date.now(),
                    result: toolResult.success ? toolResult.data : undefined,
                    error: toolResult.error,
                  },
                })}\n\n`,
              );

              flowTracker.trackEvent({
                flowId,
                stage: `text_tool_executed_iteration_${iterationCount}`,
                service: "ToolExecutor",
                status: toolResult.success ? "success" : "failed",
                duration: Date.now() - toolExecutionStart,
                data: { toolId, success: toolResult.success, toolIndex: i },
              });
            } catch (toolError) {
              console.error(
                `[ToolExecutor] Error executing text tool ${toolId}:`,
                toolError,
              );

              toolCallResults.push({
                toolCallId,
                toolId,
                success: false,
                data: null,
                error:
                  toolError instanceof Error
                    ? toolError.message
                    : String(toolError),
              });

              // Send tool_call event to frontend (error status)
              res.write(
                `data: ${JSON.stringify({
                  type: "tool_call",
                  data: {
                    id: toolCallId,
                    toolName: toolId,
                    action: params.action,
                    status: "error",
                    startTime: toolExecutionStart,
                    endTime: Date.now(),
                    error:
                      toolError instanceof Error
                        ? toolError.message
                        : String(toolError),
                  },
                })}\n\n`,
              );

              flowTracker.trackEvent({
                flowId,
                stage: `text_tool_error_iteration_${iterationCount}`,
                service: "ToolExecutor",
                status: "failed",
                data: {
                  toolId,
                  toolIndex: i,
                  error:
                    toolError instanceof Error
                      ? toolError.message
                      : String(toolError),
                },
              });
            }
          }

          // Now add the assistant message and all tool results to history
          messages.push({
            role: "assistant",
            content: assistantMessage.content || null,
            tool_calls: textToolCalls.map((tc, idx) => ({
              id: `${batchToolCallId}_${idx}`,
              type: "function",
              function: {
                name: tc.toolId,
                arguments: JSON.stringify(tc.params),
              },
            })),
          });

          // Add all tool results
          toolCallResults.forEach((result) => {
            messages.push({
              role: "tool",
              tool_call_id: result.toolCallId,
              name: result.toolId,
              content: JSON.stringify({
                success: result.success,
                data: result.data,
                error: result.error,
              }),
            });
          });

          // Track consecutive failures for text-based tool calls
          const allTextToolsFailed = toolCallResults.every((r) => !r.success);
          if (allTextToolsFailed) {
            consecutiveFailures++;
          } else {
            consecutiveFailures = 0;
          }

          // Continue to let AI process tool results
          continue;
        }

        if (
          assistantMessage.tool_calls &&
          assistantMessage.tool_calls.length > 0
        ) {
          flowTracker.trackEvent({
            flowId,
            stage: `tool_calls_detected_iteration_${iterationCount}`,
            service: "ToolExecutor",
            status: "started",
            duration: 0,
            data: {
              toolCallsCount: assistantMessage.tool_calls.length,
              tools: assistantMessage.tool_calls.map((tc) => tc.function.name),
            },
          });

          // Add assistant message with tool calls to history
          messages.push({
            role: "assistant",
            content: assistantMessage.content,
            tool_calls: assistantMessage.tool_calls,
          });

          // Execute tools in parallel
          const toolExecutionStart = Date.now();
          const toolRequests = assistantMessage.tool_calls.map((toolCall) => {
            const args = JSON.parse(toolCall.function.arguments);
            return {
              toolId: toolCall.function.name,
              action: args.action,
              params: args,
              _toolCallId: toolCall.id, // Store for response mapping
            };
          });

          // Send tool_call events for each tool (executing status)
          toolRequests.forEach((req) => {
            res.write(
              `data: ${JSON.stringify({
                type: "tool_call",
                data: {
                  id: (req as any)._toolCallId,
                  toolName: req.toolId,
                  action: req.action,
                  status: "executing",
                  startTime: toolExecutionStart,
                },
              })}\n\n`,
            );
          });

          const toolResults = await toolExecutorService.executeToolsInParallel(
            userId,
            toolRequests,
            7000, // 7s per tool
            60000, // 60s global
          );

          // Send tool_call events for each tool (success/error status)
          const endTime = Date.now();
          toolRequests.forEach((req, index) => {
            const result = toolResults[index];
            res.write(
              `data: ${JSON.stringify({
                type: "tool_call",
                data: {
                  id: (req as any)._toolCallId,
                  toolName: req.toolId,
                  action: req.action,
                  status: result.success ? "success" : "error",
                  startTime: toolExecutionStart,
                  endTime,
                  result: result.success ? result.data : undefined,
                  error: result.error,
                },
              })}\n\n`,
            );
          });

          // Track consecutive failures for circuit breaker
          const allFailed = toolResults.every((r) => !r.success);
          const someFailed = toolResults.some((r) => !r.success);
          if (allFailed) {
            consecutiveFailures++;
          } else {
            consecutiveFailures = 0; // Reset on any success
          }

          flowTracker.trackEvent({
            flowId,
            stage: `tools_executed_iteration_${iterationCount}`,
            service: "ToolExecutor",
            status: "success",
            duration: Date.now() - toolExecutionStart,
            data: {
              toolsExecuted: toolResults.length,
              successCount: toolResults.filter((r) => r.success).length,
              failureCount: toolResults.filter((r) => !r.success).length,
              consecutiveFailures,
            },
          });

          // Circuit breaker: stop if too many consecutive failures
          if (consecutiveFailures >= MAX_CONSECUTIVE_FAILURES) {
            console.warn(
              `[ChatController] Circuit breaker triggered: ${consecutiveFailures} consecutive tool failures`,
            );
            flowTracker.trackEvent({
              flowId,
              stage: `circuit_breaker_triggered`,
              service: "ChatController",
              status: "failed",
              data: { consecutiveFailures, iteration: iterationCount },
              decision: `ArrÃªt aprÃ¨s ${consecutiveFailures} Ã©checs consÃ©cutifs d'outils`,
            });
            // Don't break - let the LLM know about failures and generate a response
          }

          // Sanitize tool results before storing
          const sanitizedToolResults = toolResults.map((result) => {
            const sanitizationResult = sanitizeToolResult(result.data);
            sanitizationResults.set(result.toolUsed, sanitizationResult);

            return {
              ...result,
              data: sanitizationResult.cleaned,
              _sanitized: sanitizationResult.hasSensitiveData,
              _redactionCount: sanitizationResult.redactedCount,
            };
          });

          // Store sanitized tool results for memory
          allToolResults.push(...sanitizedToolResults);

          // Add tool results to messages
          toolRequests.forEach((req, index) => {
            const result = toolResults[index];
            messages.push({
              role: "tool",
              tool_call_id: (req as any)._toolCallId,
              name: req.toolId,
              content: JSON.stringify({
                success: result.success,
                data: result.data,
                error: result.error,
              }),
            });
          });

          // Continue loop to let AI process tool results
          continue;
        }

        // No tool calls - this is the final response
        fullResponse = assistantMessage.content || "";
        break;
      } catch (llmError) {
        // Handle max_tokens or other LLM errors
        console.error(
          `[LLM Error - Iteration ${iterationCount}]:`,
          llmError instanceof Error ? llmError.message : String(llmError),
        );

        if (isMaxTokensError(llmError)) {
          // Parse the error to get exact token information
          const overflowInfo = parseContextOverflowError(llmError);

          if (overflowInfo) {
            // We have precise information about the context overflow
            console.warn(
              `[TokenRecovery] Context overflow detected. Input: ${overflowInfo.inputTokens} tokens, ` +
                `Context limit: ${overflowInfo.contextLimit}, Available: ${overflowInfo.availableTokens}. ` +
                `Requested max_tokens was ${overflowInfo.requestedMaxTokens}, adjusting to ${overflowInfo.suggestedMaxTokens}.`,
            );

            flowTracker.trackEvent({
              flowId,
              stage: `context_overflow_recovery_${iterationCount}`,
              service: "ChatController",
              status: "started",
              data: {
                iteration: iterationCount,
                inputTokens: overflowInfo.inputTokens,
                contextLimit: overflowInfo.contextLimit,
                availableTokens: overflowInfo.availableTokens,
                requestedMaxTokens: overflowInfo.requestedMaxTokens,
                adjustedMaxTokens: overflowInfo.suggestedMaxTokens,
              },
            });

            // Strategy 0: If we have room, just adjust max_tokens and retry immediately
            if (overflowInfo.availableTokens >= 100) {
              const safeMaxTokens = calculateSafeMaxTokens(overflowInfo);
              console.log(
                `[TokenRecovery] Retrying with adjusted max_tokens: ${safeMaxTokens}`,
              );

              try {
                const recoveryResponse = await openai.chat.completions.create({
                  model: modelId,
                  messages: messages as any,
                  temperature: 0.7,
                  max_tokens: safeMaxTokens,
                  tools: toolSchemas.map((schema) => ({
                    type: "function",
                    function: schema,
                  })) as LlmTool[],
                  tool_choice: "auto",
                  stream: false,
                });

                const recoveryMessage = recoveryResponse.choices[0]?.message;
                if (recoveryMessage) {
                  // Check for tool calls in recovery response
                  if (
                    recoveryMessage.tool_calls &&
                    recoveryMessage.tool_calls.length > 0
                  ) {
                    // Add assistant message with tool calls to history
                    messages.push({
                      role: "assistant",
                      content: recoveryMessage.content,
                      tool_calls: recoveryMessage.tool_calls,
                    });
                    // Execute tools - reuse the existing tool execution logic
                    // This will continue the loop with the tool calls
                    consecutiveFailures = 0; // Reset on recovery success
                    continue;
                  }

                  fullResponse = recoveryMessage.content || "";

                  flowTracker.trackEvent({
                    flowId,
                    stage: `context_overflow_recovery_success_${iterationCount}`,
                    service: "ChatController",
                    status: "success",
                    data: {
                      adjustedMaxTokens: safeMaxTokens,
                      responseLength: fullResponse.length,
                    },
                  });

                  // Notify user about the recovery
                  try {
                    await notificationService.createNotification({
                      userId,
                      title: "Response optimized",
                      message: `Your conversation context is large. Response may be shorter than usual. Consider starting a new chat for complex queries.`,
                      type: "INFO",
                      channels: ["IN_APP"],
                    });
                  } catch (notifyError) {
                    console.warn(
                      "Failed to send recovery notification:",
                      notifyError,
                    );
                  }

                  break; // Exit with successful response
                }
              } catch (recoveryError) {
                console.warn(
                  `[TokenRecovery] Adjusted max_tokens retry failed:`,
                  recoveryError instanceof Error
                    ? recoveryError.message
                    : String(recoveryError),
                );
                // Fall through to other strategies
              }
            }

            // Strategy 1: Reduce conversation history if context is still too large
            if (messages.length > 6 && overflowInfo.availableTokens < 500) {
              console.log(
                `[TokenRecovery] Context extremely tight (${overflowInfo.availableTokens} available). ` +
                  `Reducing conversation history from ${messages.length} to essential messages.`,
              );
              // Keep system prompt + last 2 exchanges
              const systemMessages = messages.slice(0, 1);
              const recentMessages = messages.slice(-4);
              messages = [...systemMessages, ...recentMessages];
              // Continue loop with reduced context
              continue;
            }
          }

          // Fallback: Use original recovery logic for cases we couldn't parse
          console.warn(
            `[TokenFallback] Max tokens error detected. Context too large for model ${modelId}. Attempting fallback...`,
          );

          flowTracker.trackEvent({
            flowId,
            stage: `max_tokens_error_fallback_${iterationCount}`,
            service: "ChatController",
            status: "started",
            data: {
              iteration: iterationCount,
              error:
                llmError instanceof Error ? llmError.message : String(llmError),
              parsedOverflowInfo: overflowInfo,
            },
          });

          // First, try with fallback provider if configured
          if (fallbackProvider && fallbackModelId) {
            console.log(
              `[TokenFallback] Primary provider (${provider.name}) failed. Switching to fallback: ${fallbackProvider.name}`,
            );

            try {
              // Notify user about provider switch
              await notificationService.createNotification({
                userId,
                title: "Switching to alternative provider",
                message: `${provider.name} encountered issues. Attempting response with ${fallbackProvider.name}...`,
                type: "WARNING",
                channels: ["IN_APP"],
              });
            } catch (notifyError) {
              console.warn(
                "Failed to send fallback notification:",
                notifyError,
              );
            }

            flowTracker.trackEvent({
              flowId,
              stage: `fallback_provider_switch_${iterationCount}`,
              service: "ChatController",
              status: "started",
              data: {
                primaryProvider: provider.name,
                fallbackProvider: fallbackProvider.name,
                fallbackModel: fallbackModelId,
              },
            });

            // Retry with fallback
            const fallbackOpenAI = new OpenAI({
              apiKey: fallbackProvider.apiKey,
              baseURL: fallbackProvider.baseUrl || "https://api.openai.com/v1",
            });

            try {
              const fallbackResponse =
                await fallbackOpenAI.chat.completions.create({
                  model: fallbackModelId,
                  messages: messages as any,
                  temperature: 0.7,
                  max_tokens: Math.min(
                    getFallbackMaxTokens(fallbackModelId),
                    maxTokensToUse,
                  ),
                  tools: toolSchemas.map((schema) => ({
                    type: "function",
                    function: schema,
                  })) as LlmTool[],
                  tool_choice: "auto",
                  stream: false,
                });

              const fallbackMessage = fallbackResponse.choices[0]?.message;
              if (fallbackMessage) {
                fullResponse = fallbackMessage.content || "";

                flowTracker.trackEvent({
                  flowId,
                  stage: `fallback_provider_success_${iterationCount}`,
                  service: "ChatController",
                  status: "success",
                  data: {
                    responseLength: fullResponse.length,
                  },
                });

                break; // Exit the while loop with successful response
              }
            } catch (fallbackProviderError) {
              console.warn(
                `[TokenFallback] Fallback provider (${fallbackProvider.name}) also failed:`,
                fallbackProviderError,
              );
              // Fall through to existing token reduction strategies
            }
          }

          // Strategy 1: If context is too long, try to reduce conversation history
          if (messages.length > 10) {
            console.log(
              `[TokenFallback] Reducing conversation history from ${messages.length} to 5 messages`,
            );
            // Keep system prompt + last 4 messages (user + assistant pairs)
            const systemMessages = messages.slice(0, 1);
            const recentMessages = messages.slice(-4);
            messages = [...systemMessages, ...recentMessages];

            // Retry with reduced history
            continue;
          }

          // Strategy 2: If still failing, use aggressive fallback max_tokens
          const fallbackMaxTokens = getFallbackMaxTokens(modelId);
          console.log(
            `[TokenFallback] Using aggressive fallback max_tokens: ${fallbackMaxTokens}`,
          );

          try {
            const fallbackResponse = await openai.chat.completions.create({
              model: modelId,
              messages: messages as any,
              temperature: 0.7,
              max_tokens: fallbackMaxTokens,
              tools: toolSchemas.map((schema) => ({
                type: "function",
                function: schema,
              })) as LlmTool[],
              tool_choice: "auto",
              stream: false,
            });

            const fallbackMessage = fallbackResponse.choices[0]?.message;
            if (fallbackMessage) {
              // Successfully got response with fallback
              fullResponse = fallbackMessage.content || "";

              flowTracker.trackEvent({
                flowId,
                stage: `fallback_success_${iterationCount}`,
                service: "ChatController",
                status: "success",
                data: { fallbackMaxTokens },
              });

              break;
            }
          } catch (fallbackError) {
            // Fallback also failed
            console.error(
              "[TokenFallback] Fallback retry also failed:",
              fallbackError,
            );
            throw fallbackError;
          }
        } else {
          // Not a max_tokens error, re-throw
          throw llmError;
        }
      }
    }

    flowTracker.trackEvent({
      flowId,
      stage: "llm_response",
      service: "OpenAI",
      status: "success",
      duration: Date.now() - llmStart,
      data: {
        responseLength: fullResponse.length,
        iterations: iterationCount,
        toolsUsed: allToolResults.length,
      },
    });

    // 8. Stream the final response to user
    if (fullResponse) {
      // Send start event
      res.write(`data: ${JSON.stringify({ type: "start", messageId })}\n\n`);

      // Send tool usage info if any
      if (allToolResults.length > 0) {
        res.write(
          `data: ${JSON.stringify({
            type: "tools",
            data: {
              count: allToolResults.length,
              tools: allToolResults.map((r) => ({
                tool: r.toolUsed,
                success: r.success,
                duration: r.executionTime,
              })),
            },
          })}\n\n`,
        );
      }

      // Stream response in chunks (simulate streaming for consistency)
      const chunkSize = 5;
      for (let i = 0; i < fullResponse.length; i += chunkSize) {
        const chunk = fullResponse.slice(i, i + chunkSize);
        res.write(
          `data: ${JSON.stringify({ type: "token", data: chunk })}\n\n`,
        );
        // Small delay to simulate streaming
        await new Promise((resolve) => setTimeout(resolve, 10));
      }
    } else {
      // Send error notification if response is empty after all retries
      if (allToolResults.length === 0) {
        try {
          await notificationService.createNotification({
            userId,
            title: "Unable to generate response",
            message: `The conversation context was too complex for the model. Try asking a simpler question or starting a new conversation.`,
            type: "ERROR",
            channels: ["IN_APP"],
          });
        } catch (notifyError) {
          console.warn(
            "Failed to send empty response notification:",
            notifyError,
          );
        }
      }
    }

    // Complete flow and send response to user IMMEDIATELY
    const userResponseDuration = Date.now() - startTime;

    flowTracker.trackEvent({
      flowId,
      stage: "chat_complete",
      service: "ChatController",
      status: "success",
      duration: userResponseDuration,
      data: { asyncMemoryProcessing: true },
    });

    // Send end event to user - RESPONSE IS NOW COMPLETE
    res.write(`data: ${JSON.stringify({ type: "end", messageId })}\n\n`);
    res.end();

    // ===== FACT-CHECKING (ASYNC AFTER RESPONSE) =====
    // Trigger background fact-check for substantial responses
    if (fullResponse && fullResponse.length > 100) {
      setImmediate(async () => {
        try {
          const { factCheckerService } =
            await import("../services/fact-checker.js");
          await factCheckerService.scheduleFactCheck({
            userId,
            conversationId: messageId,
            messageId: messageId,
            response: fullResponse,
            userQuestion: message,
          });
        } catch (error) {
          console.error(
            "[ChatController] Fact-check scheduling failed:",
            error,
          );
          // Don't block chat on fact-check errors
        }
      });
    }

    // Update conversation cache with the exchange (Opt 2)
    responseCacheService.updateConversationContext(
      userId,
      { role: "user", content: message },
      userTopics,
    );
    responseCacheService.updateConversationContext(userId, {
      role: "assistant",
      content: fullResponse,
    });

    // 7. ASYNC: Unified analysis AFTER response (Optimization 5)
    // Single LLM call that does BOTH classification AND value assessment
    setImmediate(async () => {
      try {
        const analysisStart = Date.now();

        // Use the new unified analysis method
        const { classification, valueAssessment } =
          await intentRouter.analyzeExchangePostResponse(
            message,
            fullResponse,
            userId,
          );

        flowTracker.trackEvent({
          flowId,
          stage: "unified_analysis",
          service: "IntentRouter",
          status: "success",
          duration: Date.now() - analysisStart,
          data: {
            inputType: classification.inputType,
            confidence: classification.confidence,
            importanceScore: classification.importanceScore,
            sentiment: classification.sentiment,
            topic: classification.topic,
            entities: classification.entities,
            isValuable: valueAssessment.isValuable,
            shouldStore: valueAssessment.shouldStore,
            isFactualDeclaration: valueAssessment.isFactualDeclaration,
            asyncProcessing: true,
            singleLLMCall: true,
          },
          decision: `[ASYNC] Analyse unifiÃ©e: Type=${classification.inputType}, Importance=${(classification.importanceScore * 100).toFixed(1)}%, Stocker=${valueAssessment.shouldStore ? "OUI" : "NON"}. Raison: ${valueAssessment.reason}`,
        });

        // Only store if valuable
        if (
          valueAssessment.shouldStore &&
          valueAssessment.adjustedImportanceScore >= 0.3
        ) {
          const memoryStart = Date.now();

          // Determine what content to store
          let contentToStore: string;
          if (
            valueAssessment.isFactualDeclaration &&
            valueAssessment.factToStore
          ) {
            contentToStore = valueAssessment.factToStore;
          } else {
            contentToStore = `Question: ${message}\nRÃ©ponse: ${fullResponse}`;
          }

          // Add tool execution details to metadata only (not to stored content)
          let toolMetadata: any = {};
          if (allToolResults.length > 0) {
            // Generate summary for metadata only (not stored in main content)
            const executionSummary = generateToolExecutionSummary(
              allToolResults.map((r) => ({
                toolUsed: r.toolUsed,
                success: r.success,
                error: r.error,
                executionTime: r.executionTime,
              })),
              sanitizationResults,
            );

            // Keep technical details in metadata only, not in stored content
            // This keeps the memory focused on meaningful information

            // Create enriched metadata with success/failure counts
            toolMetadata = createToolMetadata(
              allToolResults.map((r) => ({
                toolUsed: r.toolUsed,
                success: r.success,
                error: r.error,
                executionTime: r.executionTime,
              })),
              executionSummary,
            );

            // Add sanitization info to metadata
            const sanitizationSummary = Array.from(sanitizationResults.values())
              .filter((r) => r.hasSensitiveData)
              .map((r) => ({
                redactedCount: r.redactedCount,
                redactionTypes: r.redactionSummary,
              }));

            if (sanitizationSummary.length > 0) {
              toolMetadata.sanitization = sanitizationSummary;
            }
          }

          await prisma.memory.create({
            data: {
              userId,
              content: contentToStore,
              type: "SHORT_TERM",
              sourceType: "chat",
              importanceScore: valueAssessment.adjustedImportanceScore,
              tags: classification.topic ? [classification.topic] : [],
              entities: classification.entities,
              metadata: toolMetadata.toolsUsed ? toolMetadata : undefined,
            },
          });

          flowTracker.trackEvent({
            flowId,
            stage: "memory_storage",
            service: "MemoryManager",
            status: "success",
            duration: Date.now() - memoryStart,
            data: {
              importanceScore: valueAssessment.adjustedImportanceScore,
              topic: classification.topic,
              entities: classification.entities,
              isFactualDeclaration: valueAssessment.isFactualDeclaration,
              contentStored: valueAssessment.isFactualDeclaration
                ? "fact_only"
                : "full_exchange",
              asyncProcessing: true,
            },
            decision: `[ASYNC] MÃ©moire stockÃ©e: Importance=${(valueAssessment.adjustedImportanceScore * 100).toFixed(1)}%, Type=${valueAssessment.isFactualDeclaration ? "FAIT" : "Ã‰CHANGE"}`,
          });
        } else {
          flowTracker.trackEvent({
            flowId,
            stage: "memory_storage",
            service: "MemoryManager",
            status: "skipped",
            data: { asyncProcessing: true },
            decision: `[ASYNC] Non stockÃ©: ${valueAssessment.reason}`,
          });
        }

        flowTracker.completeFlow(flowId, "completed");
      } catch (error) {
        console.warn("[ASYNC] Failed to analyze or store chat:", error);
        flowTracker.trackEvent({
          flowId,
          stage: "unified_analysis",
          service: "IntentRouter",
          status: "failed",
          error: error instanceof Error ? error.message : "Unknown error",
          data: { asyncProcessing: true },
          decision: `[ASYNC] Erreur: ${error instanceof Error ? error.message : "Unknown error"}`,
        });
        flowTracker.completeFlow(flowId, "partial");
      }
    });
  } catch (error) {
    console.error("Chat error:", error);

    flowTracker.trackEvent({
      flowId,
      stage: "chat_error",
      service: "ChatController",
      status: "failed",
      duration: Date.now() - startTime,
      error: error instanceof Error ? error.message : "Unknown error",
    });
    flowTracker.completeFlow(flowId, "failed");

    // Check if headers already sent (response may have started)
    if (!res.headersSent) {
      res.write(
        `data: ${JSON.stringify({
          type: "error",
          data:
            error instanceof Error ? error.message : "Une erreur est survenue",
        })}\n\n`,
      );
      res.write(`data: ${JSON.stringify({ type: "end", messageId })}\n\n`);
      res.end();
    }
  }
}

/**
 * Process a message from Telegram and return the AI response
 * Used by telegram.service.ts for two-way communication
 */
export async function processTelegramMessage(
  userId: string,
  message: string,
): Promise<string> {
  const startTime = Date.now();
  const flowId = randomBytes(8).toString("hex");

  try {
    console.log(
      `[Telegram] Processing message for user ${userId}: ${message.substring(0, 50)}...`,
    );

    flowTracker.startFlow(flowId, "chat");
    flowTracker.trackEvent({
      flowId,
      stage: "telegram_received",
      service: "Telegram",
      status: "started",
      data: { messageLength: message.length },
    });

    // Get provider configuration
    const providerData = await getChatProvider(userId);
    if (!providerData) {
      return "âŒ AI provider not configured. Please set up your AI settings in the web interface.";
    }

    const { provider, modelId } = providerData;

    // Validate model ID isn't a numeric database ID
    if (/^\d+$/.test(modelId)) {
      console.error(
        `[Telegram] Invalid model ID: "${modelId}". Model configuration appears corrupted.`,
      );
      return "âŒ Your AI model configuration appears to be corrupted. Please reconfigure your AI settings.";
    }

    // Initialize OpenAI client
    const openai = new OpenAI({
      apiKey: provider.apiKey,
      baseURL: provider.baseUrl,
    });

    // Search for relevant memories
    let memoryContext = "";
    try {
      const searchResults = await memorySearchService.semanticSearch(
        userId,
        message,
        5,
      );
      const results = Array.isArray(searchResults)
        ? searchResults
        : searchResults?.results || [];
      if (results.length > 0) {
        memoryContext = results
          .map((m: any) => `- ${m.memory?.content || m.content}`)
          .join("\n");
      }
    } catch (error) {
      console.warn("[Telegram] Failed to search memories:", error);
    }

    // Build system prompt with context
    let systemPrompt = CHAT_SYSTEM_PROMPT;
    if (memoryContext) {
      systemPrompt += `\n\nMÃ‰MOIRES PERTINENTES:\n${memoryContext}`;
    }

    const systemPromptWithContext = await injectContextIntoPrompt(
      systemPrompt,
      userId,
    );

    const maxTokens = await getDefaultMaxTokens(userId);
    const toolSchemas =
      (await toolExecutorService.getToolSchemasWithGenerated(
        userId,
      )) as ToolFunctionDefinition[];
    const llmTools: LlmTool[] = toolSchemas.map((schema) => ({
      type: "function",
      function: schema,
    }));

    const messages: ChatMessageParam[] = [
      { role: "system", content: systemPromptWithContext },
      { role: "user", content: message },
    ];

    let finalResponse = "";
    let iterationCount = 0;
    let consecutiveFailures = 0;
    const MAX_ITERATIONS = 10;
    const MAX_CONSECUTIVE_FAILURES = 4;

    while (iterationCount < MAX_ITERATIONS) {
      iterationCount++;

      const messagesStr = messages
        .map((m) =>
          typeof m.content === "string"
            ? m.content.substring(0, 1000)
            : "",
        )
        .join(" ");

      const validation = validateMaxTokens(
        maxTokens,
        modelId,
        messages.length,
        messagesStr,
      );
      let tokensForCall = validation.maxTokens;
      if (validation.warning) {
        console.warn(`[Telegram] ${validation.warning}`);
      }

      const response = await openai.chat.completions.create({
        model: modelId,
        messages: messages as any,
        temperature: 0.7,
        max_tokens: tokensForCall,
        tools: llmTools,
        tool_choice: "auto",
        stream: false,
      });

      const assistantMessage = response.choices[0]?.message;
      if (!assistantMessage) {
        break;
      }

      const assistantContent = assistantMessage.content || "";

      const textToolCalls =
        assistantContent && (!assistantMessage.tool_calls ||
          assistantMessage.tool_calls.length === 0)
          ? extractAllTextToolCalls(assistantContent)
          : [];

      if (textToolCalls.length > 0) {
        const batchToolCallId = `telegram_text_tool_${Date.now()}_${Math.random()
          .toString(36)
          .substr(2, 9)}`;
        const toolResults = [];

        for (let i = 0; i < textToolCalls.length; i++) {
          const { toolId, params } = textToolCalls[i];
          const toolCallId = `${batchToolCallId}_${i}`;
          const action = params.action || "request";
          let result;

          try {
            result = await toolExecutorService.executeTool(userId, {
              toolId,
              action,
              params,
            });
          } catch (toolError: any) {
            result = {
              success: false,
              error:
                toolError instanceof Error
                  ? toolError.message
                  : String(toolError),
              data: null,
              executionTime: 0,
              toolUsed: toolId,
            };
          }

          const sanitized = sanitizeToolResult(result.data);
          const payload = {
            toolId,
            success: result.success,
            data: sanitized.cleaned,
            error: result.error,
          };

          toolResults.push({
            toolCallId,
            toolId,
            payload,
          });
        }

        messages.push({
          role: "assistant",
          content: assistantContent,
          tool_calls: textToolCalls.map((tc, idx) => ({
            id: `${batchToolCallId}_${idx}`,
            type: "function",
            function: {
              name: tc.toolId,
              arguments: JSON.stringify(tc.params),
            },
          })),
        });

        toolResults.forEach((result) => {
          messages.push({
            role: "tool",
            tool_call_id: result.toolCallId,
            content: JSON.stringify(result.payload),
          });
        });

        const anySuccess = toolResults.some((r) => r.payload.success);
        consecutiveFailures = anySuccess ? 0 : consecutiveFailures + 1;

        if (consecutiveFailures >= MAX_CONSECUTIVE_FAILURES) {
          finalResponse =
            "âŒ Impossible d'exÃ©cuter l'outil demandÃ©. Peux-tu reformuler ?";
          break;
        }

        continue;
      }

      if (assistantMessage.tool_calls && assistantMessage.tool_calls.length > 0) {
      type ParsedToolArgs = {
        action?: string;
        [key: string]: any;
      };

      const toolRequests = assistantMessage.tool_calls.map((toolCall: any) => {
        let args: ParsedToolArgs = {};
        try {
          args = JSON.parse(toolCall.function.arguments || "{}");
        } catch {
          args = {};
        }

        return {
          toolId: toolCall.function.name,
          action: args.action || "request",
          params: args,
          _toolCallId: toolCall.id,
        };
      });

        const toolResults = await toolExecutorService.executeToolsInParallel(
          userId,
          toolRequests,
          7000,
          60000,
        );

        messages.push({
          role: "assistant",
          content: assistantContent,
          tool_calls: assistantMessage.tool_calls,
        });

        toolResults.forEach((result, index) => {
          const request = toolRequests[index];
          const sanitized = sanitizeToolResult(result.data);
          const payload = {
            toolId: request.toolId,
            success: result.success,
            data: sanitized.cleaned,
            error: result.error,
          };

          messages.push({
            role: "tool",
            tool_call_id: request._toolCallId,
            content: JSON.stringify(payload),
          });
        });

        const anySuccess = toolResults.some((r) => r.success);
        consecutiveFailures = anySuccess ? 0 : consecutiveFailures + 1;

        if (consecutiveFailures >= MAX_CONSECUTIVE_FAILURES) {
          finalResponse =
            "âŒ Plusieurs outils ont Ã©chouÃ©. Peux-tu reformuler ou rÃ©essayer ?";
          break;
        }

        continue;
      }

      finalResponse = assistantContent;
      break;
    }

    if (!finalResponse) {
      finalResponse = "âŒ Je n'ai pas pu gÃ©nÃ©rer de rÃ©ponse pour le moment.";
    }

    console.log(`[Telegram] Response generated in ${Date.now() - startTime}ms`);

    flowTracker.trackEvent({
      flowId,
      stage: "telegram_response",
      service: "Telegram",
      status: "success",
      duration: Date.now() - startTime,
      data: { iterations: iterationCount },
    });
    flowTracker.completeFlow(flowId, "completed");

    return finalResponse;
  } catch (error: any) {
    console.error("[Telegram] Error processing message:", error);

    flowTracker.trackEvent({
      flowId,
      stage: "telegram_response",
      service: "Telegram",
      status: "failed",
      duration: Date.now() - startTime,
      error: error instanceof Error ? error.message : String(error),
    });
    flowTracker.completeFlow(flowId, "failed");

    return `âŒ Sorry, I couldn't process your message: ${error.message}`;
  }
}

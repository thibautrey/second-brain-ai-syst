#!/bin/bash

# Second Brain AI System - Local LLM Setup
# Sets up Ollama for local AI features (chat, summarization, analysis)

set -e

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

echo -e "${BLUE}ü§ñ Setting up Local LLM (Ollama)${NC}"
echo

# Function to check if command exists
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# Detect OS
OS="unknown"
if [[ "$OSTYPE" == "darwin"* ]]; then
    OS="macos"
elif [[ "$OSTYPE" == "linux-gnu"* ]]; then
    OS="linux"
fi

echo "üì¶ Installing Ollama..."

if ! command_exists ollama; then
    if [ "$OS" = "macos" ]; then
        if command_exists brew; then
            echo "   Installing via Homebrew..."
            brew install ollama
        else
            echo "   Downloading installer..."
            curl -fsSL https://ollama.ai/install.sh | sh
        fi
    else
        echo "   Installing via official installer..."
        curl -fsSL https://ollama.ai/install.sh | sh
    fi
else
    echo -e "${GREEN}‚úÖ Ollama already installed${NC}"
fi

echo
echo "üöÄ Starting Ollama service..."

# Start Ollama in background
if [ "$OS" = "macos" ]; then
    # On macOS, start as background service
    brew services start ollama 2>/dev/null || ollama serve >/dev/null 2>&1 &
else
    # On Linux, start in background
    ollama serve >/dev/null 2>&1 &
    OLLAMA_PID=$!
    echo "   Ollama started with PID $OLLAMA_PID"
fi

# Wait for Ollama to be ready
echo "   Waiting for Ollama to start..."
for i in {1..30}; do
    if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
        break
    fi
    sleep 1
done

if ! curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
    echo -e "${RED}‚ùå Failed to start Ollama. Please check your installation.${NC}"
    exit 1
fi

echo -e "${GREEN}‚úÖ Ollama is running!${NC}"
echo

echo "üì• Downloading AI model (llama3.1:3b - ~2GB)..."
echo "   This may take a few minutes depending on your internet speed..."

# Pull the model
if ollama pull llama3.1:3b; then
    echo -e "${GREEN}‚úÖ Model downloaded successfully!${NC}"
else
    echo -e "${RED}‚ùå Failed to download model. Check your internet connection.${NC}"
    exit 1
fi

echo
echo -e "${BLUE}üîß Configuration Instructions:${NC}"
echo
echo "1. Start your Second Brain AI System:"
echo -e "   ${GREEN}docker compose up --build${NC}"
echo
echo "2. Open the web interface: http://localhost:5173"
echo
echo "3. Go to Settings ‚Üí AI Configuration"
echo
echo "4. Add a new AI Provider:"
echo "   ‚Ä¢ Name: Ollama Local"
echo "   ‚Ä¢ Type: OpenAI Compatible"
echo "   ‚Ä¢ API Key: ollama (any value)"
echo "   ‚Ä¢ Base URL: http://host.docker.internal:11434/v1"
echo "   ‚Ä¢ Models: llama3.1:3b"
echo
echo "5. Configure Task Assignments:"
echo "   ‚Ä¢ Chat: Ollama Local ‚Üí llama3.1:3b"
echo "   ‚Ä¢ Routing: Ollama Local ‚Üí llama3.1:3b"
echo "   ‚Ä¢ Summarization: Ollama Local ‚Üí llama3.1:3b"
echo "   ‚Ä¢ Analysis: Ollama Local ‚Üí llama3.1:3b"
echo
echo -e "${GREEN}üéâ Local LLM setup complete!${NC}"
echo "   Your AI features will run completely offline once configured."
echo
echo -e "${YELLOW}üí° Tips:${NC}"
echo "   ‚Ä¢ Ollama runs on port 11434"
echo "   ‚Ä¢ Use 'ollama list' to see downloaded models"
echo "   ‚Ä¢ Use 'ollama pull <model>' to download other models"
echo "   ‚Ä¢ For better performance, try 'llama3.1:8b' if you have 8GB+ RAM"
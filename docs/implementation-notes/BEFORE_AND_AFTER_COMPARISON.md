# LLM Provider Resilience: Before & After Comparison

## Your Error Scenario

```
Failed to generate AI insights: Error: Both primary provider "GpuStack" 
(qwen3-coder-30b-a3b-instruct-fp8) and fallback provider "openai" (codex-mini-latest) failed. 
Primary: timeout (403 upstream request timeout). 
Fallback: model-incompatible (404 This model is only supported in v1/responses and not in v1/chat/completions.)
```

## BEFORE: System Behavior Without Learning

```
Timeline:

Attempt 1 (10:00 AM):
  â”œâ”€ Try Primary: GpuStack/qwen3-coder â†’ TIMEOUT âŒ
  â”œâ”€ Try Fallback: OpenAI/codex-mini â†’ ERROR: "404 only in v1/responses" âŒ
  â””â”€ User gets: Error message, no insights

Attempt 2 (10:05 AM) - 5 minutes later, same error:
  â”œâ”€ Try Primary: GpuStack/qwen3-coder â†’ TIMEOUT âŒ (Same error again!)
  â”œâ”€ Try Fallback: OpenAI/codex-mini â†’ ERROR: "404 only in v1/responses" âŒ (Same error again!)
  â””â”€ User gets: Same error message, frustration grows

Attempt 3 (10:10 AM) - 10 minutes later:
  â”œâ”€ Try Primary: GpuStack/qwen3-coder â†’ TIMEOUT âŒ (Still timing out!)
  â”œâ”€ Try Fallback: OpenAI/codex-mini â†’ ERROR: "404 only in v1/responses" âŒ (Still failing!)
  â””â”€ User gets: Same error AGAIN, gives up

âŒ PROBLEM:
  - No learning from repeated failures
  - Wasted API calls on known-bad combos
  - Poor user experience
  - No insight into what's broken
  - No automatic recovery mechanism
```

## AFTER: System Behavior With Learning

```
Timeline:

Attempt 1 (10:00 AM):
  â”œâ”€ Try Primary: GpuStack/qwen3-coder
  â”‚  â””â”€ TIMEOUT â†’ Record error (error_count: 1, type: timeout, transient)
  â”œâ”€ Try Fallback: OpenAI/codex-mini
  â”‚  â””â”€ ERROR: "404 only in v1/responses"
  â”‚     â””â”€ Record error (error_count: 1, type: model-incompatible, permanent)
  â”‚     â””â”€ Learn: This model doesn't work with /v1/chat/completions
  â”œâ”€ Store in DB:
  â”‚  â”œâ”€ model_compatibility_hints[gpustack/qwen3]: errorCount=1, isBlacklisted=false
  â”‚  â””â”€ model_compatibility_hints[openai/codex]: errorCount=1, unsupportedEndpoints=["/v1/chat/completions"]
  â””â”€ User gets: Error message + system is learning

Attempt 2 (10:05 AM) - 5 minutes later:
  â”œâ”€ Check Primary: Is GpuStack/qwen3 blacklisted? NO (only 1 error, need 5+)
  â”‚  â”œâ”€ Try it (might succeed if transient timeout resolved) âœ“ or TIMEOUT âŒ
  â”‚  â””â”€ Record result (success_count: 1 OR error_count: 2)
  â”œâ”€ Check Fallback: Is OpenAI/codex blacklisted? NO (still only 1-2 errors)
  â”‚  â”œâ”€ But we KNOW it doesn't work with /v1/chat/completions
  â”‚  â”œâ”€ Could try /v1/responses if implemented
  â”‚  â””â”€ Or skip to next fallback faster
  â””â”€ User gets: Better response (if Primary recovered) or faster failure

Attempt 3 (10:10 AM) - After several failed attempts:
  â”œâ”€ Check Primary: GpuStack/qwen3 error_count >= 5, success_count = 0
  â”‚  â””â”€ NOW BLACKLISTED! Skip immediately â­ï¸
  â”œâ”€ Check Fallback: OpenAI/codex error_count >= 5, success_count = 0
  â”‚  â””â”€ NOW BLACKLISTED! Skip immediately â­ï¸
  â”œâ”€ Try tertiary/other providers (if configured)
  â””â”€ User gets: Faster response, system tried working providers instead

âœ… BENEFITS:
  - Learning prevents repeated failures
  - Blacklisted providers skipped automatically
  - Faster fallback to working providers
  - Reduced wasted API calls
  - Better user experience
  - Automatic recovery if primary comes back online
  - Admin can see exactly what's broken and why
```

## Performance Comparison

### Scenario: 100 LLM Calls Over 1 Hour

#### BEFORE (No Learning)
```
Total Attempts:           200 (2 per call: primary + fallback)
Successful Calls:         30
Failed Calls:             70
Wasted on Primary:        70 (always tries timeout provider)
Wasted on Fallback:       70 (always tries incompatible endpoint)
Wasted API Calls:         140 (70 errors Ã— 2 providers)
Average Response Time:    8 seconds (tries both, both fail)
User Experience:          Frustrating (same errors repeatedly)
Admin Visibility:         None (just sees errors, no pattern)
```

#### AFTER (With Learning)
```
Total Attempts:           130 (varies: 1-2 per call)
Successful Calls:         30 (same as before)
Failed Calls:             70 (same as before)
Wasted on Primary:        20 (skipped after blacklisting in call 5)
Wasted on Fallback:       20 (skipped after blacklisting in call 5)
Wasted API Calls:         40 (only first 4-5 calls fail both)
Average Response Time:    2 seconds (skips broken after learning)
User Experience:          Better (responds faster, less repeating)
Admin Visibility:         Excellent (knows exactly what's broken, when, why)

SAVINGS: 100 API calls (71% reduction in wasted calls!)
IMPROVEMENT: 4x faster response time after learning
```

## Error Tracking Example

### For Your Specific Error

**GpuStack Provider - qwen3-coder-30b-a3b-instruct-fp8:**

```
BEFORE:
- Log: "timeout"
- No additional context
- No action taken
- Try again next time

AFTER:
- Error type: "timeout"
- Status: 403
- Classified as: transient (might recover)
- Not blacklisted immediately (transient errors happen)
- Record: errorCount incremented, lastErrorTime set
- If continues: Will be blacklisted if error_count >= 5
- Next attempt: Check if blacklisted before trying
- System can auto-recover if provider comes back online
```

**OpenAI Provider - codex-mini-latest:**

```
BEFORE:
- Log: "404 model-incompatible"
- No additional context
- Try again next time (same error!)

AFTER:
- Error type: "model-incompatible"
- Status: 404
- Message: "only in v1/responses and not in v1/chat/completions"
- Classified as: permanent, not retryable
- Learn: This model doesn't support /v1/chat/completions endpoint
- Record: unsupportedEndpoints: ["/v1/chat/completions"]
- Alternative: Could use /v1/responses instead (future improvement)
- Next attempt: Know this endpoint doesn't work, try alternative or fallback
- Prevents: Hitting same broken endpoint repeatedly
```

## Database View After Learning

### After Multiple Attempts:

```sql
SELECT * FROM model_compatibility_hints;

providerId          | modelId                            | errorCount | successCount | isBlacklisted | lastErrorType        | unsupportedEndpoints
--------------------|------------------------------------|-----------:|-------------:|-----------------|----------------------|-----------------------
provider-gpustack   | qwen3-coder-30b-a3b-instruct-fp8 | 5          | 0            | true            | timeout              | []
provider-openai     | codex-mini-latest                 | 5          | 0            | true            | model-incompatible   | ["/v1/chat/completions"]
provider-openai     | gpt-4o-mini                      | 0          | 42           | false           | null                 | []
provider-azure      | gpt-4-turbo                      | 1          | 38           | false           | rate-limit           | []
```

### Admin Dashboard View:

```
ğŸ“Š Provider Health Dashboard
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”´ GpuStack (qwen3-coder-30b-a3b-instruct-fp8)
   Status: BLACKLISTED
   Success Rate: 0% (0/5)
   Last Error: timeout (403 upstream request timeout)
   Action: Currently disabled, will retry periodically
   
ğŸ”´ OpenAI (codex-mini-latest)
   Status: BLACKLISTED  
   Success Rate: 0% (0/5)
   Last Error: model-incompatible (endpoint incompatibility)
   Unsupported: /v1/chat/completions
   Action: Currently disabled, could try /v1/responses

ğŸŸ¢ OpenAI (gpt-4o-mini)
   Status: HEALTHY
   Success Rate: 100% (42/42)
   Last Success: 2 minutes ago
   Action: Primary provider for most tasks

ğŸŸ¡ Azure (gpt-4-turbo)
   Status: OPERATIONAL
   Success Rate: 97% (38/39)
   Last Error: rate-limit (recovered automatically)
   Action: Fallback provider

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Key Insights:
âœ“ 2 providers working reliably (gpt-4o-mini, gpt-4-turbo)
âš ï¸ 2 providers blacklisted (GpuStack, codex-mini)
â†’ Recommendation: Replace broken providers to improve overall reliability
```

## Real-World Impact

### Story 1: User at 10:00 AM

**Without Learning:**
```
User: "Generate insights about my week"
System: 
  â”œâ”€ Primary fails: "timeout"
  â”œâ”€ Fallback fails: "model-incompatible"
  â””â”€ Error: "Both providers failed"
User (angry): "Still broken? I gave up after the first error!"
```

**With Learning:**
```
User: "Generate insights about my week"
System (1st attempt):
  â”œâ”€ Primary fails: "timeout" â†’ Record error
  â”œâ”€ Fallback fails: "incompatible" â†’ Record error + learn endpoint
  â””â”€ Error: "Providers failed, will be smarter next time"
User (understanding): "Okay, trying again later..."
```

### Story 2: User at 10:05 AM (Same Error Recurring)

**Without Learning:**
```
User: "Try again"
System: (Exact same sequence as before)
  â”œâ”€ Primary fails: "timeout"
  â”œâ”€ Fallback fails: "model-incompatible"
  â””â”€ Error: Same as before
User (furious): "WHY DIDN'T YOU LEARN?!"
```

**With Learning:**
```
User: "Try again"
System (2nd attempt):
  â”œâ”€ Primary still failing, error_count: 2, not blacklisted yet
  â”œâ”€ Fallback still failing, error_count: 2, but we KNOW the endpoint is bad
  â”œâ”€ Try alternative approach (tertiary provider if exists)
  â””â”€ If providers fixed: Attempt and succeed!
User (satisfied): "It's working now!" (or "They're still fixing it")
```

### Story 3: After 4-5 Failed Attempts

**Without Learning:**
```
System: (Keeps trying same two providers over and over)
Timeline: 10:00, 10:05, 10:10, 10:15, 10:20...
Each attempt: Waste API calls, frustrate user, learn nothing
```

**With Learning:**
```
System: 
  After 5 failures each, blacklists both providers
  Subsequent attempts skip them immediately
  Falls through to working providers (if available)
  OR gracefully degrades with cached/estimated insights
Timeline: 10:00-10:20 learning, 10:21+ actually using working providers
Result: Much faster to "something works" or "nothing works"
```

## Technical Changes Summary

| Aspect | Before | After |
|--------|--------|-------|
| **Error Recording** | Console log only | Database record + log |
| **Error Tracking** | Single message | Type, timestamp, count, message, endpoint info |
| **Success Tracking** | Not tracked | Increment counter, record timestamp |
| **Blacklisting** | Manual only | Automatic after 5 errors + 0 successes |
| **Fallback Logic** | Always try both | Check blacklist, skip if known-bad |
| **Endpoint Awareness** | None | Extract from error messages, record in DB |
| **Admin Visibility** | Logs only | Full database visibility, reports possible |
| **Performance** | Same (all attempts) | Faster (skips known-bad) |
| **User Experience** | Repetitive errors | Learning and improvement |

## Files Changed

- âœ… **`backend/services/model-compatibility-hint.ts`** - NEW: Core learning system
- âœ… **`backend/services/llm-router.ts`** - UPDATED: Integrated learning into flow
- âœ… **`backend/prisma/schema.prisma`** - UPDATED: Added ModelCompatibilityHint table
- âœ… **`backend/prisma/migrations/20260128_add_model_compatibility_hints/`** - NEW: Database migration

## Getting Started

1. **Deploy** the changes using [`DEPLOYMENT_CHECKLIST.md`](DEPLOYMENT_CHECKLIST.md)
2. **Monitor** using provided SQL queries in [`llm-provider-resilience-system.md`](llm-provider-resilience-system.md)
3. **Review** learning using examples in [`llm-resilience-usage-examples.ts`](llm-resilience-usage-examples.ts)

---

**Impact**: Significantly improves resilience and user experience
**Complexity**: Low (transparent, non-breaking)
**Value**: High (reduces wasted API calls, improves response times, enables better debugging)


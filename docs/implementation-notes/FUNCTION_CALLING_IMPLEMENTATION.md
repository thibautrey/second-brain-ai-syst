# Function Calling & Multi-Tool Execution - Implementation

**Date**: 23 janvier 2026  
**Statut**: âœ… ImplÃ©mentÃ©  
**Architecture**: Streaming Hybride avec OpenAI Function Calling

---

## ğŸ“‹ Vue d'ensemble

ImplÃ©mentation du systÃ¨me d'appels d'outils automatiques permettant Ã  l'IA de dÃ©tecter quand elle a besoin d'outils et de les appeler automatiquement, y compris en parallÃ¨le.

### Exemple d'usage

**User**: "Quel temps fera-t-il demain ?"

**L'IA va automatiquement**:

1. Appeler `user_context` (get_location) pour rÃ©cupÃ©rer la localisation
2. Appeler `curl` pour interroger une API mÃ©tÃ©o
3. **Les 2 appels se font en parallÃ¨le** si possibles
4. SynthÃ©tiser les rÃ©sultats dans une rÃ©ponse naturelle

---

## ğŸ—ï¸ Architecture ImplÃ©mentÃ©e

### Mode Hybride Streaming

```
User Message
    â†“
Memory Search + Provider Config (parallÃ¨le)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BOUCLE TOOL CALLING (max 3 itÃ©rations) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
LLM Call (non-streaming) avec tools schemas
    â†“
Tool calls dÃ©tectÃ©s ?
â”œâ”€ OUI
â”‚  â”œâ”€ ExÃ©cution PARALLÃˆLE des outils
â”‚  â”‚  â”œâ”€ Timeout individuel: 7s par outil
â”‚  â”‚  â””â”€ Timeout global: 60s pour tous
â”‚  â”œâ”€ Injection rÃ©sultats dans messages
â”‚  â””â”€ Retour Ã  LLM Call (nouvelle itÃ©ration)
â”‚
â””â”€ NON â†’ RÃ©ponse finale
    â†“
STREAMING de la rÃ©ponse finale Ã  l'utilisateur
    â†“
Stockage mÃ©moire (async) avec tool results
```

### Avantages

âœ… **Simple & fiable**: Non-streaming pour tool detection  
âœ… **UX fluide**: Streaming de la rÃ©ponse finale  
âœ… **Performance**: ExÃ©cution parallÃ¨le des outils  
âœ… **Robustesse**: Timeouts individuels + global  
âœ… **TraÃ§abilitÃ©**: Tool results stockÃ©s en mÃ©moire

---

## ğŸ› ï¸ Modifications EffectuÃ©es

### 1. Tool Executor Service ([tool-executor.ts](../../backend/services/tool-executor.ts))

#### Nouvel outil: `user_context`

Permet Ã  l'IA de rÃ©cupÃ©rer des informations sur l'utilisateur depuis la mÃ©moire.

**Actions**:

- `get_location`: Recherche la localisation de l'utilisateur
- `get_preferences`: RÃ©cupÃ¨re les prÃ©fÃ©rences (optionnel: sur un topic)
- `search_facts`: Recherche des faits spÃ©cifiques

**Exemple d'appel**:

```json
{
  "name": "user_context",
  "arguments": {
    "action": "get_location"
  }
}
```

**RÃ©ponse**:

```json
{
  "action": "get_location",
  "found": true,
  "results": [
    {
      "content": "J'habite Ã  Paris, 75001",
      "score": 0.95,
      "date": "2026-01-20T10:30:00Z"
    }
  ]
}
```

#### Nouvelle mÃ©thode: `executeToolsInParallel()`

**Signature**:

```typescript
async executeToolsInParallel(
  userId: string,
  requests: ToolExecutionRequest[],
  individualTimeout: number = 7000,  // 7s par outil
  globalTimeout: number = 60000,     // 60s total
): Promise<ToolExecutionResult[]>
```

**FonctionnalitÃ©s**:

- ExÃ©cution parallÃ¨le avec `Promise.allSettled()`
- Timeout individuel par outil (7s default)
- Timeout global pour tous les outils (60s default)
- Fallback automatique: si un outil Ã©choue, les autres continuent
- Gestion d'erreur individuelle sans blocage

**Exemple**:

```typescript
const results = await toolExecutorService.executeToolsInParallel(userId, [
  { toolId: "user_context", action: "get_location", params: {} },
  {
    toolId: "curl",
    action: "get",
    params: { url: "https://api.weather.com/..." },
  },
]);
// Les 2 outils s'exÃ©cutent en parallÃ¨le
```

#### SchÃ©ma OpenAI Function Calling

Ajout du schÃ©ma `user_context` pour OpenAI:

```typescript
{
  name: "user_context",
  description: "Retrieve user context information from memory...",
  parameters: {
    type: "object",
    properties: {
      action: {
        type: "string",
        enum: ["get_location", "get_preferences", "search_facts"]
      },
      topic: { type: "string" },      // Pour get_preferences
      query: { type: "string" },      // Pour search_facts
      limit: { type: "number" }       // Pour search_facts
    },
    required: ["action"]
  }
}
```

---

### 2. Chat Controller ([chat.controller.ts](../../backend/controllers/chat.controller.ts))

#### Boucle de Tool Calling

**Structure**:

```typescript
let messages: ChatMessage[] = [
  { role: "system", content: systemPrompt },
  { role: "user", content: message }
];

let allToolResults: any[] = [];
let iterationCount = 0;
const MAX_ITERATIONS = 3;

while (iterationCount < MAX_ITERATIONS) {
  iterationCount++;

  // LLM call avec tools
  const response = await openai.chat.completions.create({
    model: modelId,
    messages: messages,
    tools: toolSchemas.map(s => ({ type: "function", function: s })),
    tool_choice: "auto",
    stream: false  // Non-streaming pour dÃ©tecter tool_calls
  });

  const assistantMessage = response.choices[0]?.message;

  // Check for tool calls
  if (assistantMessage.tool_calls?.length > 0) {
    // Add assistant message to history
    messages.push({
      role: "assistant",
      content: assistantMessage.content,
      tool_calls: assistantMessage.tool_calls
    });

    // Execute tools in parallel
    const toolResults = await toolExecutorService.executeToolsInParallel(...);

    // Add results to messages
    toolResults.forEach((result, index) => {
      messages.push({
        role: "tool",
        tool_call_id: toolCalls[index].id,
        name: toolCalls[index].function.name,
        content: JSON.stringify(result)
      });
    });

    // Continue loop
    continue;
  }

  // No tool calls - final response
  fullResponse = assistantMessage.content;
  break;
}
```

#### Streaming de la rÃ©ponse finale

AprÃ¨s la boucle, streaming simulÃ© pour cohÃ©rence UX:

```typescript
// Send tool usage info
if (allToolResults.length > 0) {
  res.write(
    `data: ${JSON.stringify({
      type: "tools",
      data: {
        count: allToolResults.length,
        tools: allToolResults.map((r) => ({
          tool: r.toolUsed,
          success: r.success,
          duration: r.executionTime,
        })),
      },
    })}\n\n`,
  );
}

// Stream response in chunks
const chunkSize = 5;
for (let i = 0; i < fullResponse.length; i += chunkSize) {
  const chunk = fullResponse.slice(i, i + chunkSize);
  res.write(`data: ${JSON.stringify({ type: "token", data: chunk })}\n\n`);
  await new Promise((resolve) => setTimeout(resolve, 10));
}
```

#### Stockage mÃ©moire enrichi

Les tool results sont inclus dans le contenu de la mÃ©moire:

```typescript
// Add tool results to content
if (allToolResults.length > 0) {
  const toolsSummary = allToolResults
    .filter((r) => r.success)
    .map((r) => `- ${r.toolUsed}: ${JSON.stringify(r.data).substring(0, 100)}`)
    .join("\n");

  contentToStore += `\n\nOutils utilisÃ©s:\n${toolsSummary}`;
}

await prisma.memory.create({
  data: {
    userId,
    content: contentToStore,
    type: "SHORT_TERM",
    sourceType: "chat",
    importanceScore: valueAssessment.adjustedImportanceScore,
    tags: classification.topic ? [classification.topic] : [],
    entities: classification.entities,
    metadata: {
      toolsUsed: allToolResults.map((r) => r.toolUsed),
      toolResultsCount: allToolResults.length,
    },
  },
});
```

---

## ğŸ“Š Ã‰vÃ©nements SSE pour le Frontend

Le systÃ¨me envoie maintenant des Ã©vÃ©nements enrichis:

### `type: "start"`

```json
{ "type": "start", "messageId": "msg_123" }
```

### `type: "tools"` (nouveau)

```json
{
  "type": "tools",
  "data": {
    "count": 2,
    "tools": [
      { "tool": "user_context", "success": true, "duration": 234 },
      { "tool": "curl", "success": true, "duration": 1523 }
    ]
  }
}
```

### `type: "token"`

```json
{ "type": "token", "data": "chunk" }
```

### `type: "end"`

```json
{ "type": "end", "messageId": "msg_123" }
```

---

## ğŸ” Flow Tracking

Nouveaux Ã©vÃ©nements de tracking:

```typescript
// Iteration tracking
flowTracker.trackEvent({
  flowId,
  stage: `llm_call_iteration_${iterationCount}`,
  service: "OpenAI",
  status: "started",
  data: { messagesCount: messages.length },
});

// Tool detection
flowTracker.trackEvent({
  flowId,
  stage: `tool_calls_detected_iteration_${iterationCount}`,
  service: "ToolExecutor",
  data: {
    toolCallsCount: assistantMessage.tool_calls.length,
    tools: assistantMessage.tool_calls.map((tc) => tc.function.name),
  },
});

// Tool execution
flowTracker.trackEvent({
  flowId,
  stage: `tools_executed_iteration_${iterationCount}`,
  service: "ToolExecutor",
  status: "success",
  duration: executionTime,
  data: {
    toolsExecuted: toolResults.length,
    successCount: toolResults.filter((r) => r.success).length,
    failureCount: toolResults.filter((r) => !r.success).length,
  },
});
```

---

## ğŸ¯ Exemples de ScÃ©narios

### ScÃ©nario 1: Simple question (pas d'outils)

**User**: "Comment vas-tu ?"

**Flow**:

1. LLM call â†’ Pas de tool_calls dÃ©tectÃ©s
2. RÃ©ponse directe: "Je vais bien, merci !"
3. 1 itÃ©ration seulement

### ScÃ©nario 2: MÃ©tÃ©o avec localisation

**User**: "Quel temps fera-t-il demain ?"

**Flow**:

1. **Iteration 1**: LLM dÃ©tecte besoin de 2 outils
   - `user_context` (get_location)
   - `curl` (API mÃ©tÃ©o)
2. ExÃ©cution parallÃ¨le (< 2s total car parallÃ¨le)
3. **Iteration 2**: LLM reÃ§oit les rÃ©sultats
   - Location: "Paris, France"
   - MÃ©tÃ©o: { temp: 15, condition: "cloudy" }
4. LLM gÃ©nÃ¨re rÃ©ponse: "Demain Ã  Paris, il fera 15Â°C avec un temps nuageux"
5. Streaming de la rÃ©ponse finale

**SSE Events**:

```
type: start
type: tools â†’ count: 2, tools: [user_context, curl]
type: token â†’ "Demain"
type: token â†’ " Ã  Pa"
type: token â†’ "ris, "
...
type: end
```

### ScÃ©nario 3: Recherche complexe (chaÃ®ne d'outils)

**User**: "Quelle est la tempÃ©rature Ã  l'endroit oÃ¹ j'ai vÃ©cu l'annÃ©e derniÃ¨re ?"

**Flow**:

1. **Iteration 1**: `user_context` (search_facts: "vÃ©cu l'annÃ©e derniÃ¨re")
2. **Iteration 2**: `curl` (API mÃ©tÃ©o pour la ville trouvÃ©e)
3. **Iteration 3**: RÃ©ponse finale avec synthÃ¨se

---

## âš™ï¸ Configuration

### Timeouts

```typescript
// Dans executeToolsInParallel()
const INDIVIDUAL_TIMEOUT = 7000; // 7s par outil
const GLOBAL_TIMEOUT = 60000; // 60s total

// Dans chat.controller.ts
await toolExecutorService.executeToolsInParallel(
  userId,
  toolRequests,
  7000, // Timeout individuel
  60000, // Timeout global
);
```

### Limite d'itÃ©rations

```typescript
const MAX_ITERATIONS = 3; // Ã‰vite les boucles infinies
```

---

## ğŸ§ª Tests RecommandÃ©s

### Test 1: Simple tool call

```
User: "Quelle est ma localisation ?"
Expected: Appel user_context â†’ RÃ©ponse avec localisation
```

### Test 2: Parallel tools

```
User: "Quel temps fera-t-il demain ?"
Expected: user_context + curl en parallÃ¨le â†’ RÃ©ponse mÃ©tÃ©o
```

### Test 3: Tool chain

```
User: "CrÃ©e une tÃ¢che pour me rappeler la mÃ©tÃ©o demain"
Expected: curl (mÃ©tÃ©o) â†’ todo (crÃ©er tÃ¢che)
```

### Test 4: Timeout handling

```
User: "Appelle une API trÃ¨s lente"
Expected: Timeout aprÃ¨s 7s â†’ Erreur gracieuse
```

### Test 5: No tools needed

```
User: "Bonjour !"
Expected: RÃ©ponse directe sans outils (1 itÃ©ration)
```

---

## ğŸš€ Prochaines Ã‰tapes

1. âœ… **ImplÃ©mentÃ©**: Function calling avec mode hybride
2. âœ… **ImplÃ©mentÃ©**: ExÃ©cution parallÃ¨le avec timeouts
3. âœ… **ImplÃ©mentÃ©**: Outil user_context
4. âœ… **ImplÃ©mentÃ©**: Stockage mÃ©moire enrichi
5. ğŸ”„ **Ã€ tester**: VÃ©rifier le systÃ¨me avec exemples rÃ©els
6. ğŸ“ **Ã€ venir**: Frontend UI pour afficher les tool calls
7. ğŸ“ **Ã€ venir**: Analytics sur l'usage des outils
8. ğŸ“ **Ã€ venir**: Optimisations de performance

---

## ğŸ“š RÃ©fÃ©rences

- [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)
- [chat.controller.ts](../../backend/controllers/chat.controller.ts)
- [tool-executor.ts](../../backend/services/tool-executor.ts)
- [memory-search.ts](../../backend/services/memory-search.ts)

---

**DerniÃ¨re mise Ã  jour**: 23 janvier 2026  
**ImplÃ©mentÃ© par**: GitHub Copilot  
**Statut**: âœ… PrÃªt pour tests
